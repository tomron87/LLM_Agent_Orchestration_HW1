# üß≠ Self-Evaluation Guide for AI Agent Course Projects

This guide outlines the stages and criteria for self-evaluation of your software project submission. Completing it is optional, but highly recommended before submitting your assignment.

---

## **Stage 1: Understanding the Criteria and Standards**

Before you can assess your work, make sure you fully understand the criteria on which it will be evaluated:

- Carefully read the **submission and software guidelines**.
- Identify all required components (documentation, code, tests, analysis, etc.).
- Understand the **expected quality level** for each criterion.
- Pay attention to distinctions between different quality levels.

---

## **Stage 2: Mapping Your Work to the Criteria (Checklist)**

### **Project Documentation ‚Äì 20%**
**PRD (Product Requirements Document):**
- Clear description of project purpose and user problem.
- Measurable goals and KPIs.
- Detailed functional and non-functional requirements.
- Dependencies, assumptions, and limitations.
- Project timeline and milestones.

**Architecture Documentation:**
- Block diagrams (C4 Model, UML).
- Operational architecture.
- Architectural decision records (ADRs).
- API and interface documentation.

**Self-assigned score:** ____ / 20

---

### **README and Code Documentation ‚Äì 15%**
**Comprehensive README:**
- Step-by-step installation instructions.
- Detailed execution guide.
- Example runs and screenshots.
- Configuration guide.
- Troubleshooting section.

**Code Comments and Docstrings:**
- Docstrings for every function/class/module.
- Explanations for complex design decisions.
- Descriptive and consistent naming conventions.

**Self-assigned score:** ____ / 15

---

### **Project Structure and Code Quality ‚Äì 15%**
**Project Organization:**
- Modular and clear folder structure (`src/`, `tests/`, `docs/`, `data/`, `results/`, `config/`, `assets/`).
- Separation between code, data, and results.
- Files limited to ~150 lines.
- Consistent naming conventions.

**Code Quality:**
- Short, focused functions (Single Responsibility).
- Avoid duplicate code (DRY principle).
- Consistent coding style.

**Self-assigned score:** ____ / 15

---

### **Configuration and Security ‚Äì 10%**
**Configuration Management:**
- Separate configuration files (`.env`, `.yaml`, `.json`).
- No hardcoded constants.
- Example config file (`.env.example`).
- Parameter documentation.

**Security:**
- No API keys in source code.
- Proper use of environment variables.
- Updated `.gitignore` file.

**Self-assigned score:** ____ / 10

---

### **Testing and Quality Assurance ‚Äì 15%**
**Test Coverage:**
- Unit tests with ‚â•70% coverage for new code.
- Edge case testing.
- Coverage reports.

**Error Handling:**
- Documented edge cases with clear responses.
- Comprehensive error handling.
- Clear and informative error messages.
- Debugging logs.

**Test Results:**
- Documented expected outcomes.
- Automated testing reports.

**Self-assigned score:** ____ / 15

---

### **Research and Analysis ‚Äì 15%**
**Experiments and Parameters:**
- Systematic experiments with parameter tuning.
- Sensitivity analysis.
- Tables with experimental results.
- Identification of critical parameters.

**Analysis Notebook:**
- Jupyter Notebook or equivalent.
- Methodical and deep analysis.
- Mathematical formulas in LaTeX (if applicable).
- References to academic literature.

**Visual Presentation:**
- High-quality plots (bar, line, heatmaps, etc.).
- Clear labels and legends.
- High resolution and readable visuals.

**Self-assigned score:** ____ / 15

---

### **UI/UX and Extensibility ‚Äì 10%**
**User Interface:**
- Clear and intuitive user interface.
- Screenshots and documented workflows.
- Accessibility considerations.

**Extensibility:**
- Defined extension points/hooks.
- Documented plugin development process.
- Clear modular interfaces.

**Self-assigned score:** ____ / 10

---

## **Stage 3: Depth and Uniqueness Analysis**
Answer the following to evaluate the depth and originality of your project:

### **Technical Depth**
- Did I use advanced AI agent techniques?
- Did I include theoretical or mathematical analysis?
- Did I perform comparative research between approaches?

### **Originality and Innovation**
- Does the project include original or innovative ideas?
- Did I solve a complex or challenging problem?
- Did I add value beyond the basic requirements?

### **Prompt Engineering Documentation**
- Did I document the development process using AI?
- Did I include examples of significant prompts?
- Did I include best practices learned from experience?

### **Cost and Optimization**
- Did I calculate token usage or API cost?
- Did I present a detailed cost table?
- Did I propose optimization strategies?

---

## **Stage 4: Self-Scoring Guide (Performance Levels)**

### **Level 1: Score 60‚Äì69 (Basic Pass)**
**Description:** Meets minimum submission requirements.

**Characteristics:**
- Functional code that completes the required tasks.
- Basic documentation (README with setup and usage).
- Reasonable but imperfect structure.
- Limited or partial test coverage.
- Results exist but without deep analysis.

**Evaluation style:** Flexible and forgiving, focused on reasoning and effort.

**Recommendation:** Choose this level if the project works but is incomplete or rushed.

---

### **Level 2: Score 70‚Äì79 (Good)**
**Description:** A solid and well-documented project.

**Characteristics:**
- Clean modular code with comments.
- Good documentation (README, basic PRD, architecture overview).
- Well-organized structure separating code, data, and results.
- Tests with 50‚Äì70% coverage.
- Basic result analysis with plots.
- Proper configuration and API key management.

**Evaluation style:** Balanced and fair, focusing on main criteria.

**Recommendation:** Choose this if your project is complete and well-structured.

---

### **Level 3: Score 80‚Äì89 (Very Good)**
**Description:** High academic quality work.

**Characteristics:**
- Professional modular code with clear separation of concerns.
- Full documentation (comprehensive PRD, C4 architecture, detailed README).
- Perfect structure following best practices.
- Extensive tests (70‚Äì85% coverage).
- In-depth research and sensitivity analysis.
- Clear and high-quality visualizations.
- Optimized costs and documented efficiency.

**Evaluation style:** Thorough and detailed, focusing on full criteria alignment.

**Recommendation:** Choose this if you covered all requirements comprehensively and produced strong analytical work.

---

### **Level 4: Score 90‚Äì100 (Outstanding Excellence)**
**Description:** MIT-level, publication-grade or industry-standard project.

**Characteristics:**
- Production-grade code with extensibility, hooks, and plugin architecture.
- Fully detailed PRD, architecture, and README.
- Full compliance with ISO/IEC 25010 standards.
- 85%+ test coverage with documented edge cases.
- Deep research: theoretical analysis, proofs, or comparative studies.
- Exceptional visualization (interactive dashboards).
- Comprehensive Prompt Book with best practices.
- Complete cost and optimization analysis.
- High innovation and originality.
- Community contribution (open source, reusable documentation).

**Evaluation style:** Extremely strict and detail-oriented (‚Äúsearching for elephants in a straw‚Äù).

**Recommendation:** Choose this only if your project is flawless, innovative, and ready for publication or industry use.

---

**Author:** Course Faculty  
**Adapted by:** AI Agent Expert  
**Purpose:** To enable precise self-evaluation and consistent grading of software projects.
